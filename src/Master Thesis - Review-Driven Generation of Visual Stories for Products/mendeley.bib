@article{Firth1957,
author = {Firth, John R},
journal = {Studies in linguistic analysis},
publisher = {Basil Blackwell},
title = {{A synopsis of linguistic theory, 1930-1955}},
year = {1957}
}
@article{Young,
archivePrefix = {arXiv},
arxivId = {1708.02709},
author = {Young, Tom and Hazarika, Devamanyu and Poria, Soujanya and Cambria, Erik},
eprint = {1708.02709},
journal = {CoRR},
title = {{Recent Trends in Deep Learning Based Natural Language Processing}},
url = {http://arxiv.org/abs/1708.02709},
volume = {abs/1708.0},
year = {2017}
}
@article{Wu2017,
archivePrefix = {arXiv},
arxivId = {1709.03856},
author = {Wu, Ledell and Fisch, Adam and Chopra, Sumit and Adams, Keith and Bordes, Antoine and Weston, Jason},
eprint = {1709.03856},
journal = {CoRR},
title = {{StarSpace: Embed All The Things!}},
url = {http://arxiv.org/abs/1709.03856},
volume = {abs/1709.0},
year = {2017}
}
@inproceedings{Ren2013,
address = {New York, NY, USA},
author = {Ren, Zhaochun and Liang, Shangsong and Meij, Edgar and de Rijke, Maarten},
booktitle = {Proceedings of the 36th International ACM SIGIR Conference on Research and Development in Information Retrieval},
doi = {10.1145/2484028.2484052},
isbn = {978-1-4503-2034-4},
keywords = {data enrichment,topic modeling,tweets summarization,twitter},
pages = {513--522},
publisher = {ACM},
series = {SIGIR '13},
title = {{Personalized Time-aware Tweets Summarization}},
url = {http://doi.acm.org/10.1145/2484028.2484052},
year = {2013}
}
@inproceedings{Pennington,
author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
booktitle = {Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)},
pages = {1532--1543},
title = {{Glove: Global vectors for word representation}},
year = {2014}
}
@inproceedings{Nielsen1990,
address = {New York, NY, USA},
author = {Nielsen, Jakob and Molich, Rolf},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
doi = {10.1145/97243.97281},
isbn = {0-201-50932-6},
pages = {249--256},
publisher = {ACM},
series = {CHI '90},
title = {{Heuristic Evaluation of User Interfaces}},
url = {http://doi.acm.org/10.1145/97243.97281},
year = {1990}
}
@inproceedings{Mikolov2013a,
author = {Mikolov, Tomas and Yih, Wen-tau and Zweig, Geoffrey},
booktitle = {Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
pages = {746--751},
title = {{Linguistic regularities in continuous space word representations}},
year = {2013}
}
@article{MicrosoftCanada2015,
author = {Gausby, Alyson and Others},
journal = {Microsoft Canada},
title = {{Attention Spans. Consumer Insights}},
year = {2015}
}
@inproceedings{Hu2004,
author = {Hu, Minqing and Liu, Bing},
booktitle = {AAAI},
number = {4},
pages = {755--760},
title = {{Mining opinion features in customer reviews}},
volume = {4},
year = {2004}
}
@article{Liu2017,
abstract = {Product reviews have become an important resource for customers before they make purchase decisions. However, the abundance of reviews makes it difficult for customers to digest them and make informed choices. In our study, we aim to help customers who want to quickly capture the main idea of a lengthy product review before they read the details. In contrast with existing work on review analysis and document summarization, we aim to retrieve a set of real-world user questions to summarize a review. In this way, users would know what questions a given review can address and they may further read the review only if they have similar questions about the product. Specifically, we design a two-stage approach which consists of question selection and question diversification. For question selection phase, we first employ probabilistic retrieval models to locate candidate questions that are relevant to a given review. A Recurrent Neural Network Encoder--Decoder is utilized to measure the ``answerability'' of questions to a review. We then design a set function to re-rank the questions with the goal of rewarding diversity in the final question set. The set function satisfies submodularity and monotonicity, which results in an efficient greedy algorithm of submodular optimization. Evaluation on product reviews from two categories shows that the proposed approach is effective for discovering meaningful questions that are representative of individual reviews.},
author = {Liu, Mengwen and Fang, Yi and Choulos, Alexander G and Park, Dae Hoon and Hu, Xiaohua},
doi = {10.1007/s10791-017-9311-0},
issn = {1573-7659},
journal = {Information Retrieval Journal},
month = {dec},
number = {6},
pages = {575--605},
title = {{Product review summarization through question retrieval and diversification}},
url = {https://doi.org/10.1007/s10791-017-9311-0},
volume = {20},
year = {2017}
}
@inproceedings{Hu2004a,
address = {New York, NY, USA},
author = {Hu, Minqing and Liu, Bing},
booktitle = {Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
doi = {10.1145/1014052.1014073},
isbn = {1-58113-888-1},
keywords = {reviews,sentiment classification,summarization,text mining},
pages = {168--177},
publisher = {ACM},
series = {KDD '04},
title = {{Mining and Summarizing Customer Reviews}},
url = {http://doi.acm.org/10.1145/1014052.1014073},
year = {2004}
}
@article{Method2014,
archivePrefix = {arXiv},
arxivId = {1402.3722},
author = {Goldberg, Yoav and Levy, Omer},
eprint = {1402.3722},
journal = {CoRR},
title = {{word2vec Explained: deriving Mikolov et al.'s negative-sampling word-embedding method}},
url = {http://arxiv.org/abs/1402.3722},
volume = {abs/1402.3},
year = {2014}
}
@article{Conneau2017,
archivePrefix = {arXiv},
arxivId = {1710.04087},
author = {Conneau, Alexis and Lample, Guillaume and Ranzato, Marc'Aurelio and Denoyer, Ludovic and J{\'{e}}gou, Herv{\'{e}}},
eprint = {1710.04087},
journal = {CoRR},
title = {{Word Translation Without Parallel Data}},
url = {http://arxiv.org/abs/1710.04087},
volume = {abs/1710.0},
year = {2017}
}
@article{Bojanowski2016,
archivePrefix = {arXiv},
arxivId = {1607.04606},
author = {Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
eprint = {1607.04606},
journal = {CoRR},
title = {{Enriching Word Vectors with Subword Information}},
url = {http://arxiv.org/abs/1607.04606},
volume = {abs/1607.0},
year = {2016}
}
@inproceedings{Al-Dhelaan2017,
address = {New York, NY, USA},
author = {Al-Dhelaan, Mohammed and Al-Suhaim, Abeer},
booktitle = {Proceedings of the International Conference on Web Intelligence},
doi = {10.1145/3106426.3106525},
isbn = {978-1-4503-4951-2},
keywords = {opinion summarization,review summarization,sentiment-aware summarization,text summarization},
pages = {723--729},
publisher = {ACM},
series = {WI '17},
title = {{Sentiment Diversification for Short Review Summarization}},
url = {http://doi.acm.org/10.1145/3106426.3106525},
year = {2017}
}
@inbook{Aggarwal2012,
abstract = {Clustering is a widely studied data mining problem in the text domains. The problem finds numerous applications in customer segmentation, classification, collaborative filtering, visualization, document organization, and indexing. In this chapter, we will provide a detailed survey of the problem of text clustering. We will study the key challenges of the clustering problem, as it applies to the text domain. We will discuss the key methods used for text clustering, and their relative advantages. We will also discuss a number of recent advances in the area in the context of social network and linked data.},
address = {Boston, MA},
author = {Aggarwal, Charu C and Zhai, ChengXiang},
booktitle = {Mining Text Data},
doi = {10.1007/978-1-4614-3223-4_4},
isbn = {978-1-4614-3223-4},
pages = {77--128},
publisher = {Springer US},
title = {{A Survey of Text Clustering Algorithms}},
url = {https://doi.org/10.1007/978-1-4614-3223-4{\_}4},
year = {2012}
}
@article{Zahalka2018,
author = {Zah{\'{a}}lka, J and Rudinac, S and J{\'{o}}nsson, B and Koelma, D and Worring, M},
doi = {10.1109/TMM.2017.2755986},
issn = {1520-9210},
journal = {IEEE Transactions on Multimedia},
keywords = {Internet,computer aided instruction,data compressi},
month = {mar},
number = {3},
pages = {687--698},
title = {{Blackthorn: Large-Scale Interactive Multimodal Learning}},
volume = {20},
year = {2018}
}
@article{Zahalka2015,
author = {Zah{\'{a}}lka, J and Rudinac, S and Worring, M},
doi = {10.1109/TMM.2015.2480007},
issn = {1520-9210},
journal = {IEEE Transactions on Multimedia},
keywords = {feature extraction,interactive systems,learning (a},
month = {dec},
number = {12},
pages = {2235--2244},
title = {{Interactive Multimodal Learning for Venue Recommendation}},
volume = {17},
year = {2015}
}
@inproceedings{VanDenBerg,
abstract = {The potential of mining tourist information from social multimedia data gives rise to new applications offering much richer impressions of the city. In this paper we propose Scenemash, a system that generates multimodal summaries of multiple alternative routes between locations in a city. To get insight into the geographic areas on the route, we collect a dataset of community-contributed images and their associated annotations from Foursquare and Flickr. We identify images and terms representative of a geographic area by jointly analysing distributions of a large number of semantic concepts detected in the visual content and latent topics extracted from associated text. Scenemash prototype is implemented as an Android app for smartphones and smartwatches.},
address = {Cham},
author = {van den Berg, Jorrit and Rudinac, Stevan and Worring, Marcel},
booktitle = {Advances in Information Retrieval},
editor = {Ferro, Nicola and Crestani, Fabio and Moens, Marie-Francine and Mothe, Josiane and Silvestri, Fabrizio and {Di Nunzio}, Giorgio Maria and Hauff, Claudia and Silvello, Gianmaria},
isbn = {978-3-319-30671-1},
pages = {833--836},
publisher = {Springer International Publishing},
title = {{Scenemash: Multimodal Route Summarization for City Exploration}},
year = {2016}
}
@article{Das2007,
abstract = {The increasing availability of online information has necessitated intensive research in the area of automatic text summarization within the Natural Lan- guage Processing (NLP) community. Over the past half a century, the prob- lem has been addressed from many different perspectives, in varying domains and using various paradigms. This survey intends to investigate some of the most relevant approaches both in the areas of single-document and multiple- document summarization, giving special emphasis to empirical methods and extractive techniques. Some promising approaches that concentrate on specific details of the summarization problem are also discussed. Special attention is devoted to automatic evaluation of summarization systems, as future research on summarization is strongly dependent on progress in this area.},
archivePrefix = {arXiv},
arxivId = {7182216},
author = {Das, Dipanjan and Martins, Andr{\'{e}} F.T.},
doi = {10.1016/B0-08-044854-2/00957-3},
eprint = {7182216},
file = {:Users/marc/Dropbox/Master/Thesis/References/Summarization/Dipanjan Das - A Survey on Automatic Text Summarization.pdf:pdf},
isbn = {9780080448541},
issn = {0080448542},
journal = {Eighth ACIS International Conference on Software Engineering Artificial Intelligence Networking and ParallelDistributed Computing SNPD 2007},
pages = {574--578},
pmid = {4520227},
title = {{A Survey on Automatic Text Summarization}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4287749},
volume = {4},
year = {2007}
}
@article{Deerwester1990,
abstract = {applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Deerwester, Scott and Dumais, Susan T. and Furnas, George W and Landauer, Thomas K and Harshman, Richard},
doi = {10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9},
eprint = {arXiv:1011.1669v3},
file = {:Users/marc/Dropbox/Master/Thesis/References/Embeddings/GloVe/Scott Deerwester - Indexing by Latent Semantic Analysis.pdf:pdf},
isbn = {9788578110796},
issn = {10974571},
journal = {Journal of the American Society for Information Science},
number = {6},
pages = {391--407},
pmid = {25246403},
title = {{Indexing by latent semantic analysis}},
volume = {41},
year = {1990}
}
@article{Harris2015,
abstract = {Harris maintains that it is possible to define a linguistic structure solely in terms of the "distributions" (= patterns of co-occurrences) of its elements. There is no parallel meaning-structure which can aid in describing formal structure. Meaning is partly a function of distribution.},
author = {Harris, Zellig S},
doi = {10.1080/00437956.1954.11659520},
file = {:Users/marc/Dropbox/Master/Thesis/References/Embeddings/Zellig S. Harris - Distributional Structure.pdf:pdf},
isbn = {978-90-277-1267-7},
issn = {0043-7956},
journal = {WORD},
number = {2-3},
pages = {146--162},
title = {{Distributional Structure}},
url = {http://www.tandfonline.com/doi/full/10.1080/00437956.1954.11659520},
volume = {10},
year = {1954}
}
@article{Mikolov,
abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several improvements that make the Skip-gram model more expressive and enable it to learn higher quality vectors more rapidly. We show that by subsampling frequent words we obtain significant speedup, and also learn higher quality representations as measured by our tasks. We also introduce Negative Sampling, a simplified variant of Noise Contrastive Estimation (NCE) that learns more accurate vectors for frequent words compared to the hierarchical softmax. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of Canada'' and "Air'' cannot be easily combined to obtain "Air Canada''. Motivated by this example, we present a simple and efficient method for finding phrases, and show that their vector representations can be accurately learned by the Skip-gram model.},
archivePrefix = {arXiv},
arxivId = {1310.4546},
author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
doi = {10.1162/jmlr.2003.3.4-5.951},
eprint = {1310.4546},
file = {:Users/marc/Dropbox/Master/Thesis/References/Embeddings/word2vec/Tomas Mikolov - Distributed Representations of Words and Phrases and their Compositionality.pdf:pdf},
isbn = {2150-8097},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems 26 (NIPS 2013)},
pages = {1--9},
pmid = {903},
title = {{Distributed Representations of Words and Phrases and their Compositionality}},
volume = {26},
year = {2013}
}
@article{Levy2015,
abstract = {Recent trends suggest that neural-network-inspired word embedding models outperform traditional count-based distri-butional models on word similarity and analogy detection tasks. We reveal that much of the performance gains of word embeddings are due to certain system design choices and hyperparameter op-timizations, rather than the embedding algorithms themselves. Furthermore, we show that these modifications can be transferred to traditional distributional models, yielding similar gains. In contrast to prior reports, we observe mostly local or insignificant performance differences between the methods, with no global advantage to any single approach over the others.},
archivePrefix = {arXiv},
arxivId = {1103.0398},
author = {Levy, Omer and Goldberg, Yoav and Dagan, Ido},
doi = {10.1186/1472-6947-15-S2-S2},
eprint = {1103.0398},
file = {:Users/marc/Dropbox/Master/Thesis/References/Embeddings/Omer Levy - Improving Distributional Similarity with Lessons Learned from Word Embeddings.pdf:pdf},
isbn = {1472-6947 (Electronic)$\backslash$r1472-6947 (Linking)},
issn = {2307-387X},
journal = {Transactions of the Association for Computational Linguistics},
pages = {211--225},
pmid = {26099735},
title = {{Improving Distributional Similarity with Lessons Learned from Word Embeddings}},
url = {https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/570},
volume = {3},
year = {2015}
}
@article{Marshall1996,
abstract = {The probability sampling techniques used for quantitative studies are rarely appropriate when conducting qualitative research. This article considers and explains the differences between the two approaches and describes three broad categories of naturalistic sampling: convenience, judgement and theoretical models. The principles are illustrated with practical examples from the author's own research.},
author = {Marshall, Martin N},
doi = {10.1093/fampra/13.6.522},
file = {:Users/marc/Dropbox/Master/Thesis/References/User Testing/Martin N. Marshall - Sampling for qualitative research.pdf:pdf},
isbn = {0263-2136 U6 - ctx{\_}ver=Z39.88-2004{\&}ctx{\_}enc=info{\%}3Aofi{\%}2Fenc{\%}3AUTF-8{\&}rfr{\_}id=info:sid/summon.serialssolutions.com{\&}rft{\_}val{\_}fmt=info:ofi/fmt:kev:mtx:journal{\&}rft.genre=article{\&}rft.atitle=Sampling+for+qualitative+research{\&}rft.jtitle=Family+practice{\&}rft.au=Marsh},
issn = {02632136},
journal = {Family Practice},
keywords = {qualitative sampling},
number = {6},
pages = {522--525},
pmid = {9023528},
title = {{Sampling for qualitative research Sample size}},
volume = {13},
year = {1996}
}
@article{Harrower2011,
abstract = {Choosing effective colour schemes for thematic maps is surprisingly difficult. {\{}ColorBrewer{\}} is an online tool designed to take some of the guesswork out of this process by helping users select appropriate colour schemes for their specific mapping needs by considering: the number of data classes; the nature of their data (matched with sequential, diverging and qualitative schemes); and the end-use environment for the map (e.g., {\{}CRT{\}}, {\{}LCD{\}}, printed, projected, photocopied). {\{}ColorBrewer{\}} contains 'learn more' tutorials to help guide users, prompts them to test-drive colour schemes as both map and legend, and provides output in five colour specification systems.},
author = {Harrower, Mark and Brewer, Cynthia A.},
doi = {10.1002/9780470979587.ch34},
file = {:Users/marc/Dropbox/Master/Thesis/References/Mark Harrower - ColorBrewer.org - An Online Tool for Selecting Colour Schemes for Maps.pdf:pdf},
isbn = {9780470742839},
issn = {0008-7041},
journal = {The Map Reader: Theories of Mapping Practice and Cartographic Representation},
keywords = {35 colour scheme 'sets' - in ColorBrewer,Choosing number of data classes - important part o,ColorBrewer, designed - guesswork out of process,ColorBrewer, online tool - taking guesswork out of,ColorBrewer.org - online tool for selecting colour,Colour, central role in thematic cartography,Dimensions, three of visual variables - or basic ',Diverging colour schemes - critical data class or,Map as diagnostic tool - problem with choropleth m,Sequential colour schemes},
pages = {261--268},
title = {{ColorBrewer.org: An Online Tool for Selecting Colour Schemes for Maps}},
volume = {7041},
year = {2011}
}
@article{Tilkov2010,
abstract = {One of the more interesting developments recently gaining popularity in the server-side JavaScript space is Node.js. It's a framework for developing high-performance, concurrent programs that don't rely on the mainstream multithreading approach but use asynchronous I/O with an event-driven programming model.},
author = {Tilkov, Stefan and Vinoski, Steve},
doi = {10.1109/MIC.2010.145},
file = {:Users/marc/Dropbox/Master/Thesis/References/Stefan Tilkov - Node.js - Using JavaScript to Build High-Performance Network Programs.pdf:pdf},
isbn = {1089-7801},
issn = {10897801},
journal = {IEEE Internet Computing},
keywords = {Internet,JavaScript,Node,Node.js,Web development,functional programming},
number = {6},
pages = {80--83},
title = {{Node.js: Using JavaScript to build high-performance network programs}},
volume = {14},
year = {2010}
}
@article{Pedregosa2012,
abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
archivePrefix = {arXiv},
arxivId = {1201.0490},
author = {Pedregosa, Fabian and Varoquaux, Ga{\"{e}}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, {\'{E}}douard},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {1201.0490},
file = {:Users/marc/Dropbox/Master/Thesis/References/Clustering/Fabian Pedregosa - Scikit-learn - Machine Learning in Python.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {Journal of Machine Learning Research},
pages = {2825--2830},
pmid = {1000044560},
title = {{Scikit-learn: Machine Learning in Python}},
url = {http://dl.acm.org/citation.cfm?id=2078195{\%}5Cnhttp://arxiv.org/abs/1201.0490},
volume = {12},
year = {2012}
}
@article{Manning2009,
abstract = {Class-tested and coherent, this groundbreaking new textbook teaches web-era information retrieval, including web search and the related areas of text classification and text clustering from basic concepts. Written from a computer science perspective by three leading experts in the field, it gives an up-to-date treatment of all aspects of the design and implementation of systems for gathering, indexing, and searching documents; methods for evaluating systems; and an introduction to the use of machine learning methods on text collections. All the important ideas are explained using examples and figures, making it perfect for introductory courses in information retrieval for advanced undergraduates and graduate students in computer science. Based on feedback from extensive classroom experience, the book has been carefully structured in order to make teaching more natural and effective. Although originally designed as the primary text for a graduate or advanced undergraduate course in information retrieval, the book will also create a buzz for researchers and professionals alike.},
archivePrefix = {arXiv},
arxivId = {0521865719 9780521865715},
author = {Manning, Christopher D. and Ragahvan, Prabhakar and Schutze, Hinrich},
doi = {10.1109/LPT.2009.2020494},
eprint = {0521865719 9780521865715},
file = {:Users/marc/Dropbox/Master/Thesis/References/Preprocessing/Christopher D. Manning - An Introduction to Information Retrieval.pdf:pdf},
isbn = {0521865719},
issn = {13864564},
journal = {Information Retrieval},
number = {c},
pages = {1--18},
pmid = {10575050},
title = {{An Introduction to Information Retrieval}},
year = {2009}
}
@article{Porter1980,
abstract = {The automatic removal of suffixes from words in English is of particular interest in the field of information retrieval. An algorithm for suffix stripping is described, which has been implemented as a short, fast program in BCPL....},
archivePrefix = {arXiv},
arxivId = {http://dx.doi.org/10.1108/BIJ-10-2012-0068},
author = {Porter, M. F.},
doi = {10.1108/eb046814},
eprint = {/dx.doi.org/10.1108/BIJ-10-2012-0068},
file = {:Users/marc/Dropbox/Master/Thesis/References/Preprocessing/M.F. Porter - An algorithm for suffix stripping.pdf:pdf},
isbn = {1558604545},
issn = {00330337},
journal = {Program},
keywords = {computer applications,historical research,information retrieval,paper type technical paper},
number = {3},
pages = {130--137},
pmid = {16143652},
primaryClass = {http:},
title = {{An algorithm for suffix stripping}},
volume = {14},
year = {1980}
}
@misc{Christensson2017,
abstract = {Stands for "Relational Database Management System." An RDBMS is a DBMS designed specifically for relational databases. Therefore, RDBMSes are a subset of DBMSes.},
author = {Christensson, P.},
title = {{RDBMS (Relational Database Management System) Definition}},
url = {https://techterms.com/definition/rdbms},
urldate = {2018-07-16},
year = {2017}
}
@misc{Christensson2006,
abstract = {Stands for "Database Management System." In short, a DBMS is a database program. Technically speaking, it is a software system that uses a standard method of cataloging, retrieving, and running queries on data. The DBMS manages incoming data, organizes it, and provides ways for the data to be modified or extracted by users or other programs.},
author = {Christensson, P.},
title = {{DBMS (Database Management System) Definition}},
url = {https://techterms.com/definition/dbms},
urldate = {2018-07-16},
year = {2006}
}
@misc{Christensson2015,
abstract = {Stands for "Garbage In, Garbage Out." GIGO is a computer science acronym that implies bad input will result in bad output.},
author = {Christensson, P.},
title = {{GIGO (Garbage In, Garbage Out) Definition}},
url = {https://techterms.com/definition/gigo},
urldate = {2018-07-16},
year = {2015}
}
@article{Mcauley,
abstract = {Most online reviews consist of plain-text feedback together with a single numeric score. However, there are multiple dimensions to products and opinions, and understanding the 'aspects' that contribute to users' ratings may help us to better understand their individual preferences. For example, a user's impression of an audiobook presumably depends on aspects such as the story and the narrator, and knowing their opinions on these aspects may help us to recommend better products. In this paper, we build models for rating systems in which such dimensions are explicit, in the sense that users leave separate ratings for each aspect of a product. By introducing new corpora consisting of five million reviews, rated with between three and six aspects, we evaluate our models on three prediction tasks: First, we uncover which parts of a review discuss which of the rated aspects. Second, we summarize reviews, by finding the sentences that best explain a user's rating. Finally, since aspect ratings are optional in many of the datasets we consider, we recover ratings that are missing from a user's evaluation. Our model matches state-of-the-art approaches on existing small-scale datasets, while scaling to the real-world datasets we introduce. Moreover, our model is able to 'disentangle' content and sentiment words: we automatically learn content words that are indicative of a particular aspect as well as the aspect-specific sentiment words that are indicative of a particular rating.},
archivePrefix = {arXiv},
arxivId = {arXiv:1210.3926v2},
author = {Mcauley, Julian and Leskovec, Jure and Stanford, Dan Jurafsky},
eprint = {arXiv:1210.3926v2},
file = {:Users/marc/Dropbox/Master/Thesis/References/RateBeer/Julian McAuley - Learning Attitudes and Attributes from Multi-Aspect Reviews.pdf:pdf},
keywords = {-machine learning,segmentation,summarization},
title = {{Learning Attitudes and Attributes from Multi-Aspect Reviews}}
}
@article{McAuley2013,
abstract = {Recommending products to consumers means not only understanding their tastes, but also understanding their level of experience. For example, it would be a mistake to recommend the iconic film Seven Samurai simply because a user enjoys other action movies; rather, we might conclude that they will eventually enjoy it -- once they are ready. The same is true for beers, wines, gourmet foods -- or any products where users have acquired tastes: the `best' products may not be the most `accessible'. Thus our goal in this paper is to recommend products that a user will enjoy now, while acknowledging that their tastes may have changed over time, and may change again in the future. We model how tastes change due to the very act of consuming more products -- in other words, as users become more experienced. We develop a latent factor recommendation system that explicitly accounts for each user's level of experience. We find that such a model not only leads to better recommendations, but also allows us to study the role of user experience and expertise on a novel dataset of fifteen million beer, wine, food, and movie reviews.},
archivePrefix = {arXiv},
arxivId = {1303.4402},
author = {McAuley, Julian and Leskovec, Jure},
doi = {10.1145/2488388.2488466},
eprint = {1303.4402},
file = {:Users/marc/Dropbox/Master/Thesis/References/RateBeer/Julian McAuley - From Amateurs to Connoisseurs- Modeling the Evolution of User Expertise through Online Reviews.pdf:pdf},
isbn = {9781450320351},
keywords = {expertise,recommender systems,user modeling},
title = {{From Amateurs to Connoisseurs: Modeling the Evolution of User Expertise through Online Reviews}},
url = {http://arxiv.org/abs/1303.4402},
year = {2013}
}
@article{Peterson2009,
abstract = {K-nearest-neighbor (kNN) classification is one of the most fundamental and simple classification methods and should be one of the first choices for a classification study when there is little or no prior knowledge about the distribution of the data.},
author = {Peterson, Leif},
doi = {10.4249/scholarpedia.1883},
isbn = {1941-6016},
issn = {1941-6016},
journal = {Scholarpedia},
number = {2},
pages = {1883},
title = {{K-nearest neighbor}},
url = {http://www.scholarpedia.org/article/K-nearest{\_}neighbor},
volume = {4},
year = {2009}
}
@article{Obeidat2010,
abstract = {Usability is no longer a forgotten quality attribute. Producing usable software user interface designs has become one of the important goals of software developers. Usability problems are intolerable as they affect software systems negatively. This paper proposes an approach to solve user interface usability problems by using a mix of user interface design guidelines and adaptation techniques.},
author = {Obeidat, Mo'ath Zaid Abedalaziz and Siti, Salwah Salim},
doi = {10.1109/ICACTE.2010.5579015},
file = {:Users/marc/Dropbox/Master/Thesis/References/User Testing/Integrating{\_}User{\_}Interface{\_}Design{\_}Guidelines{\_}With{\_}Adaptation{\_}Techniques{\_}to{\_}Solve{\_}Usability{\_}Problems-1.pdf:pdf},
isbn = {9781424465408},
issn = {2154-7491},
journal = {ICACTE 2010 - 2010 3rd International Conference on Advanced Computer Theory and Engineering, Proceedings},
keywords = {Adaptation techniques,Usability,User interface guidelines},
pages = {280--284},
title = {{Integrating user interface design guidelines with adaptation techniques to solve usability problems}},
volume = {1},
year = {2010}
}
@article{Costa2008,
author = {Costa, Pedro J},
doi = {10.1145/1456536.1456591},
file = {:Users/marc/Dropbox/Master/Thesis/References/User Testing/Evaluating{\_}Web{\_}Site{\_}Design.pdf:pdf},
isbn = {978-1-60558-083-8},
journal = {Proceedings of the 26th Annual ACM International Conference on Design of Communication},
keywords = {aesthetics,design principles,web,web design},
pages = {265--266},
title = {{Evaluating Web Site Design}},
url = {http://doi.acm.org/10.1145/1456536.1456591},
year = {2008}
}
@article{Salman2018,
author = {Salman, Hasanin Mohammed and {Wan Ahmad}, Wan Fatimah and Sulaiman, Suziah},
doi = {10.1007/978-3-319-59427-9_17},
file = {:Users/marc/Dropbox/Master/Thesis/References/User Testing/Usability{\_}Evaluation{\_}of{\_}the{\_}Smartphone{\_}User{\_}Interface{\_}in{\_}Supporting{\_}Elderly{\_}Users{\_}From{\_}Experts{\_}Perspective.pdf:pdf},
issn = {21693536},
journal = {IEEE Access},
pages = {155--162},
title = {{Revisiting the Usability of Smartphone User Interface for Elderly Users}},
url = {http://link.springer.com/10.1007/978-3-319-59427-9{\_}17},
volume = {6},
year = {2018}
}
@article{Harris2009,
abstract = {This paper explores the meaning of ethical Web design. It defines a set of guidelines as to what could help define ethical Web design and gives real world examples for justification. These examples provide excellent case studies for a Web design class.},
author = {Harris, James K.},
file = {:Users/marc/Dropbox/Master/Thesis/References/User Testing/Ethical{\_}Issues{\_}Web{\_}Design.pdf:pdf},
issn = {1937-4771},
journal = {Journal of Computing Sciences in Colleges},
keywords = {ethics,privacy,web design},
number = {2},
pages = {214--220},
title = {{Ethical issues in web design}},
volume = {25},
year = {2009}
}
@article{Nakamichi2006,
abstract = {The purpose of this research is to detect low usability web pages from the behavior of users, such as browsing time, mouse movement and eye movement. We experimented to investigate the relation between the quantitative data viewing behavior of users and web usability evaluation by subjects. We analyzed the data to detect low usability web pages using discriminant analysis. Low usability web pages, 94.4{\%} (17pages / 18pages = detectable pages / low usability pages) were detectable from the moving speed of gazing points and the amount of wheel rolling of a mouse. Moreover, this detection reduced the number of web pages which should be evaluated by half (46{\%} = 89 pages / 192 pages = detected pages / all pages).},
author = {Nakamichi, Noboru and Shima, Kazuyuki and Sakai, Makoto and Matsumoto, Ken-ichi},
doi = {10.1145/1134285.1134365},
file = {:Users/marc/Dropbox/Master/Thesis/References/User Testing/Detecting{\_}Low{\_}Usability{\_}Web{\_}Pages{\_}using{\_}Quantitative{\_}Data{\_}of{\_}Users{\_}Behavior.pdf:pdf},
isbn = {159593085X},
issn = {02705257},
journal = {Proceedings of the 28th international conference on Software engineering},
keywords = {evaluation,eye information,gazing point,performance,web usability},
pages = {569--576},
title = {{Detecting low usability web pages using quantitative data of users' behavior}},
url = {http://dl.acm.org/citation.cfm?id=1134365},
year = {2006}
}
@article{Ebling2000,
abstract = {Many sources of empirical data can be used to evaluate an interface (e.g., time to learn, time to perform benchmark tasks, number of errors on benchmark tasks, answers on questionnaires, comments made in verbal protocols). This paper examines the relative contributions of both quantitative and qualitative data gathered during a usability study. For each usability problem uncovered by this study, we trace each contributing piece of evidence back to its empirical source. For this usability study, the verbal protocol provided the sole source of evidence for more than one third of the most severe problems and more than two thirds of the less severe problems. Thus, although the verbal protocol provided the bulk of the evidence, other sources of data contributed disproportionately to the more critical problems. This work suggests that further research is required to determine the relative value of different forms of empirical evidence.},
author = {Ebling, Maria R and John, Bonnie E},
doi = {10.1145/347642.347766},
file = {:Users/marc/Dropbox/Master/Thesis/References/User Testing/On{\_}the{\_}Contributions{\_}of{\_}Different{\_}Emperical{\_}Data{\_}in{\_}Usability{\_}Testing.pdf:pdf},
isbn = {1581132190},
journal = {Proceedings of the Conference on Designing Interactive Systems Processes, Practices, Methods, and Techniques},
keywords = {empirical data,usability testing,verbal protocol},
pages = {289--296},
title = {{On the Contributions of Different Empirical Data in Usability Testing}},
url = {http://portal.acm.org/citation.cfm?doid=347642.347766},
year = {2000}
}
@article{Mikolov2013,
abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
archivePrefix = {arXiv},
arxivId = {1301.3781},
author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
doi = {10.1162/153244303322533223},
eprint = {1301.3781},
file = {:Users/marc/Dropbox/Master/Thesis/References/Embeddings/word2vec/Tomas Mikolov - Efficient Estimation of Word Representations in Vector Space.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
pages = {1--12},
pmid = {18244602},
title = {{Efficient Estimation of Word Representations in Vector Space}},
url = {http://arxiv.org/abs/1301.3781},
year = {2013}
}
@article{Haas2017,
abstract = {Universiteit Utrecht},
author = {de Haas, L.M.},
file = {:Users/marc/Dropbox/Master/Thesis/References/Summarization/Lucas de Haas - Extractive Summarization using Sentence Embeddings.pdf:pdf},
keywords = {Artificial Intelligence,Blendle,embeddings,extractive summarization,neural network,recurrent neural network,sentence embeddings,sequence2sequence,word2vec},
title = {{EXTRACTIVE SUMMARIZATION USING SENTENCE EMBEDDINGS: Automatic summarization of news articles at Blendle}},
url = {https://dspace.library.uu.nl/handle/1874/357917},
year = {2017}
}
@article{Yang2009,
abstract = {As the number of transactions in E-market places is growing, more and more product information and product reviews are posted on the Internet. Because customers want to purchase good products, product reviews became most important information. But, because of the massive volume of reviews, customers canpsilat read all reviews. In order to solve this problem, a lot of research is being carried out in Opinion Mining. Through the Opinion Mining, we can know about contents of whole product reviews. Traditionally research on natural language processing was applied to the opinion mining area in early stage. Recently, the computational statistics are applied to handle massive volume of reviews. In this research, we suggest a method for summarization of product reviews using the userpsilas opinion, feature occurrences, and the rate of review in order to improve the performance of existing methods. With this method, we can handle massive volumes of reviews in a short time efficiently. We guarantee the correctness of the review summary by finding out the semantic meaning of reviews. Besides, we show these advantages through some experiments.},
author = {Yang, Jung Yeon and Myung, Jaeseok and Lee, Sang Goo},
doi = {10.1109/eKNOW.2009.15},
file = {:Users/marc/Dropbox/Master/Thesis/References/Summarization/Jung-Yeon Yang - The Method for a Summarization of Product Reviews Using the User's Opinion.pdf:pdf},
isbn = {9780769535319},
journal = {Proceedings - International Conference on Information, Process, and Knowledge Management, eKNOW 2009},
keywords = {Opinion mining,Product review,Review summary},
pages = {84--89},
title = {{The method for a summarization of product reviews using the user's opinion}},
year = {2009}
}
@article{Bal2016,
abstract = {The Distributed ASCI Supercomputer (DAS) is a Computer Science infrastructure designed by the Advanced School for Computing and Imaging (ASCI) for controlled experiments with parallel and distributed systems. We have set up five generations of DAS, each consisting of 4-6 clusters located at different Dutch universities and integrated into a single shared distributed system using an advanced wide-area network. DAS is unique in that it has been available for 18 years, but always was kept consistent with the current research agenda. Each generation was set up by a single organization and with one clear vision. The DAS systems therefore are homogeneous, consistent, and easy to maintain. The goal of this paper is to show the huge impact of such a persistent long-term investment in Computer Science infrastructure, including numerous major awards, top publications, over 100 Ph.D. theses, and highly fruitful collaborations with application domains.},
author = {Bal, Henri and Epema, Dick and {De Laat}, Cees and {Van Nieuwpoort}, Rob and Romein, John and Seinstra, Frank and Snoek, Cees and Wijshoff, Harry},
doi = {10.1109/MC.2016.127},
file = {:Users/marc/Dropbox/Master/Thesis/References/das2016.pdf:pdf},
issn = {00189162},
journal = {Computer},
keywords = {collaborative computing,computer architecture,computer science research,computing for education,distributed architectures,distributed programming,distributed systems,e-science research,experimental environments,heterogeneous computing,homogeneous system organization,network optimization,parallel architectures,research infrastructures},
number = {5},
pages = {54--63},
title = {{A Medium-Scale Distributed System for Computer Science Research: Infrastructure for the Long Term}},
volume = {49},
year = {2016}
}
@article{Lin2002,
abstract = {In this paper we discuss manual and automatic evaluations of summaries using data from the Document Understanding Conference 2001 (DUC-2001). We first show the instability of the manual evaluation. Specifically, the low inter- human agreement indicates that more reference summaries are needed. To investigate the feasibility of automated summary evaluation based on the recent BLEU method from machine translation, we use accumulative n-gram overlap scores between system and human summaries. The initial results provide encouraging correlations with human judgments, based on the Spearman rank-order correlation coefficient. However, relative ranking of systems needs to take into account the instability. 1},
author = {Lin, Chin-yew and Hovy, Eduard},
doi = {10.3115/1118162.1118168},
file = {:Users/marc/Dropbox/Master/Thesis/References/Evaluation/Chin-Yew Lin - Manual and Automatic Evaluation of Summaries.pdf:pdf},
journal = {Proceedings of the ACL02 Workshop on Automatic Summarization},
number = {July},
pages = {45--51},
title = {{Manual and automatic evaluation of summaries}},
url = {http://portal.acm.org/citation.cfm?doid=1118162.1118168},
volume = {4},
year = {2002}
}
@article{Nenkova2004,
abstract = {We present an empirically grounded method for evaluating content selection in summariza- tion. It incorporates the idea that no single best model summary for a collection of documents exists. Our method quantifies the relative im- portance of facts to be conveyed. We argue that it is reliable, predictive and diagnostic, thus im- proves considerably over the shortcomings of the human evaluation method currently used in the Document Understanding Conference.},
author = {Nenkova, a and Passonneau, R},
file = {:Users/marc/Dropbox/Master/Thesis/References/Evaluation/Ani Nenkova - Evaluating Content Selection in Summarization - The Pyramid Method.pdf:pdf},
journal = {Proceedings of HLT-NAACL},
pages = {145--152},
title = {{Evaluating content selection in summarization: The pyramid method}},
url = {papers2://publication/uuid/DC675E84-0A45-48B7-A26C-F08B4B9398D3},
volume = {2004},
year = {2004}
}
@article{Harman2002,
author = {Harman, Donna and Over, Paul},
file = {:Users/marc/Dropbox/Master/Thesis/References/Evaluation/Donna Harman - The DUC Summarization Evaluations.pdf:pdf},
journal = {Second international conference on Human Language Technology Research},
pages = {44--51},
title = {{The DUC summarization evaluations}},
year = {2002}
}
@article{Lin2004,
abstract = {ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation. It includes measures to auto- matically determine the quality of a summary by comparing it to other (ideal) summaries created by humans. The measures count the number of over- lapping units such as n-gram, word sequences, and word pairs between the computer-generated sum- mary to be evaluated and the ideal summaries cre- ated by humans. This paper introduces four different ROUGE measures: ROUGE-N, ROUGE-L, ROUGE-W, and ROUGE-S included in the ROUGE summariza- tion evaluation package and their evaluatio ns. Three of them have been used in the Document Under- standing$\backslash$tConference$\backslash$t(DUC)$\backslash$t2004,$\backslash$ta$\backslash$tlarge -scale summarization evaluation sponsored by NIST.},
author = {Lin, C Y},
file = {:Users/marc/Dropbox/Master/Thesis/References/Evaluation/Chin-Yew Lin - ROUGE - A Package for Automatic Evaluation of Summaries.pdf:pdf},
issn = {00036951},
journal = {Proceedings of the workshop on text summarization branches out (WAS 2004)},
number = {1},
pages = {25--26},
title = {{Rouge: A package for automatic evaluation of summaries}},
year = {2004}
}
@article{B2018a,
author = {B, Stevan Rudinac and Chua, Tat-seng and Diaz-ferreyra, Nicolas and Friedland, Gerald and Gornostaja, Tatjana and Huet, Benoit and Kaptein, Rianne and Lind, Krister},
doi = {10.1007/978-3-319-73603-7},
file = {:Users/marc/Dropbox/Master/Thesis/References/Summarization/Stevan Rudinac - Rethinking Summarization and Storytelling for Modern Social Multimedia.pdf:pdf},
isbn = {978-3-319-73602-0},
issn = {00200255},
journal = {International Conference on Multimedia Modeling},
pages = {632--644},
title = {{MultiMedia Modeling}},
url = {http://link.springer.com/10.1007/978-3-319-73603-7},
volume = {10704},
year = {2018}
}
@article{Rudinac2013,
abstract = {In this paper we propose a novel approach to selecting images suitable for inclusion in the visual summaries. The approach is grounded in insights about how people summarize image collections. We utilize the Amazon Mechanical Turk crowdsourcing platform to obtain a large number of manually created visual summaries as well as information about criteria for image inclusion in the summary. Based on these large-scale user tests, we propose an automatic image selection approach, which jointly utilizes the analysis of image content, context, popularity, visual aesthetic appeal as well as the sentiment derived from the comments posted on the images. In our approach we do not describe images based on their properties only, but also in the context of semantically related images, which improves robustness and effectively enables propagation of sentiment, aesthetic appeal as well as various inherent attributes associated with a particular group of images. We discuss the phenomenon of a low inter-user agreement, which makes an automatic evaluation of visual summaries a challenging task and propose a solution inspired by the text summarization and machine translation communities. The experiments performed on a collection of geo-referenced Flickr images demonstrate the effectiveness of our image selection approach.},
author = {Rudinac, Stevan and Larson, Martha and Hanjalic, Alan},
doi = {10.1109/TMM.2013.2261481},
file = {:Users/marc/Dropbox/Master/Thesis/References/Summarization/Stevan Rudinac - Learning Crowdsourced User Preferences for Visual Summarization of Image Collections.pdf:pdf},
isbn = {1520-9210 VO - 15},
issn = {15209210},
journal = {IEEE Transactions on Multimedia},
keywords = {Crowdsourcing,image aesthetic appeal,image content and context,image set evaluation,learning to rank,sentiment analysis,social media,user-informed visual summarization},
number = {6},
pages = {1231--1243},
title = {{Learning crowdsourced user preferences for visual summarization of image collections}},
volume = {15},
year = {2013}
}
@article{Nagwani2015,
abstract = {Document summarization provides an instrument for faster understanding the collection of text documents and has a number of real life applications. Semantic similarity and clustering can be utilized efficiently for generating effective summary of large text collections. Summarizing large volume of text is a challenging and time consuming problem particularly while considering the semantic similarity computation in summarization process. Summarization of text collection involves intensive text processing and computations to generate the summary. MapReduce is proven state of art technology for handling Big Data. In this paper, a novel framework based on MapReduce technology is proposed for summarizing large text collection. The proposed technique is designed using semantic similarity based clustering and topic modeling using Latent Dirichlet Allocation (LDA) for summarizing the large text collection over MapReduce framework. The summarization task is performed in four stages and provides a modular implementation of multiple documents summarization. The presented technique is evaluated in terms of scalability and various text summarization parameters namely, compression ratio, retention ratio, ROUGE and Pyramid score are also measured. The advantages of MapReduce framework are clearly visible from the experiments and it is also demonstrated that MapReduce provides a faster implementation of summarizing large text collections and is a powerful tool in Big Text Data analysis.},
author = {Nagwani, N. K.},
doi = {10.1186/s40537-015-0020-5},
file = {:Users/marc/Dropbox/Master/Thesis/References/Summarization/N K Nagwani - Summarizing large text collection using topic modeling and clustering based on MapReduce framework.pdf:pdf},
isbn = {2196-1115},
issn = {21961115},
journal = {Journal of Big Data},
keywords = {Big Text Data analysis,Clustering based summarization,Semantic similarity,Summarizing large text,Text clustering},
number = {1},
pages = {1--18},
publisher = {Journal of Big Data},
title = {{Summarizing large text collection using topic modeling and clustering based on MapReduce framework}},
url = {http://dx.doi.org/10.1186/s40537-015-0020-5},
volume = {2},
year = {2015}
}
@article{Rudinac2013a,
abstract = {In this paper, we present a novel approach for automatic visual summarization of a geographic area that exploits user-contributed images and related explicit and implicit metadata collected from popular content-sharing websites. By means of this approach, we search for a limited number of representative but diverse images to represent the area within a certain radius around a specific location. Our approach is based on the random walk with restarts over a graph that models relations between images, visual features extracted from them, associated text, as well as the information on the uploader and commentators. In addition to introducing a novel edge weighting mechanism, we propose in this paper a simple but effective scheme for selecting the most representative and diverse set of images based on the information derived from the graph. We also present a novel evaluation protocol, which does not require input of human annotators, but only exploits the geographical coordinates accompanying the images in order to reflect conditions on image sets that must necessarily be fulfilled in order for users to find them representative and diverse. Experiments performed on a collection of Flickr images, captured around 207 locations in Paris, demonstrate the effectiveness of our approach.},
author = {Rudinac, Stevan and Hanjalic, Alan and Larson, Martha},
doi = {10.1109/TMM.2013.2237896},
file = {:Users/marc/Dropbox/Master/Thesis/References/Summarization/Stevan Rudinac - Generating Visual Summaries of Geographic Areas Using Community-Contributed Images.pdf:pdf},
issn = {15209210},
journal = {IEEE Transactions on Multimedia},
keywords = {Automatic evaluation of visual summaries,graph-based models,image set diversity,image set representativeness,multimodal fusion,social media,visual summarization of geographic areas},
number = {4},
pages = {921--932},
title = {{Generating visual summaries of geographic areas using community-contributed images}},
volume = {15},
year = {2013}
}
